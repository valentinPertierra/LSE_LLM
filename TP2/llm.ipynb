{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "222fc310-3303-4d6a-9570-595d7f412601",
   "metadata": {},
   "source": [
    "## Conexión con modelo del lenguaje\n",
    "\n",
    "Se utilizara la API de Groq para realizar inferenicas en LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b221d43-030d-4345-8522-4d10c33245d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from groq import Groq\n",
    "\n",
    "from langchain.chains import ConversationChain, LLMChain\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    ")\n",
    "from langchain_core.messages import SystemMessage\n",
    "from langchain.chains.conversation.memory import ConversationBufferWindowMemory\n",
    "\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd3bad0b-f64e-4959-91ea-81d3999a2779",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "GROQ_API_KEY = os.getenv('GROQ_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f671d8e6-3061-460c-ab62-b0773bad721b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_15868\\1956692779.py:6: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferWindowMemory(k=conversational_memory_length, memory_key=\"historial_chat\", return_messages=True)\n"
     ]
    }
   ],
   "source": [
    "# Defino modelo a utilizar\n",
    "model = 'llama3-8b-8192'     # 'mixtral-8x7b-32768', 'gemma-7b-it'\n",
    "\n",
    "# Instancio memoria conversacional \n",
    "conversational_memory_length = 5\n",
    "memory = ConversationBufferWindowMemory(k=conversational_memory_length, memory_key=\"historial_chat\", return_messages=True)\n",
    "\n",
    "# Para borrar la memoria \n",
    "#memory.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c56368e-032f-40ae-93e3-115c58129ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "groq_chat = ChatGroq(\n",
    "    groq_api_key=GROQ_API_KEY, \n",
    "    model_name=model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ccbcd7f1-b3f5-4c1f-917d-93e02e72555d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_15868\\2100480735.py:20: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  conversation = LLMChain(\n"
     ]
    }
   ],
   "source": [
    "system_prompt = ''\n",
    "\n",
    "# Creo un prompt de prueba\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [   # Este es el mensaje del sistema persistente que siempre se incluye al inicio del chat.\n",
    "        SystemMessage(\n",
    "            content=system_prompt\n",
    "        ),  \n",
    "        # Este marcador de posición será reemplazado por el historial de chat real durante la conversación. Ayuda a mantener el contexto.\n",
    "        MessagesPlaceholder(\n",
    "            variable_name=\"historial_chat\"\n",
    "        ),  \n",
    "        # Esta plantilla es donde se inyectará la entrada actual del usuario en el mensaje.\n",
    "        HumanMessagePromptTemplate.from_template(\n",
    "            \"{human_input}\"\n",
    "        ),  \n",
    "    ]\n",
    ")\n",
    "\n",
    "conversation = LLMChain(\n",
    "    llm=groq_chat,  # El objeto de chat Groq LangChain inicializado anteriormente.\n",
    "    prompt=prompt,  # La plantilla de mensaje construida.\n",
    "    verbose=True,   # Habilita la salida detallada, lo cual puede ser útil para depurar.\n",
    "    memory=memory,  # El objeto de memoria conversacional que almacena y gestiona el historial de la conversación.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "536a8a31-3d52-423a-a5c9-00da3d96ec9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: \n",
      "Human: Hola! en que me podes ayudar?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "user_question = 'Hola! en que me podes ayudar?'\n",
    "response = conversation.predict(human_input=user_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e484e1f8-9a5c-43f1-8e27-2c124baad39d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hola! ¡Bienvenido! Estoy aquí para ayudarte en lo que necesites. ¿En qué puedo ayudarte hoy? ¿Tienes un problema que quieras resolver, una pregunta que te haga, o simplemente quieres charlar un rato? Estoy aquí para escucharte y ayudarte de la mejor manera posible.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a8acc81-c5e2-4be8-91d2-c65f053438f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: \n",
      "Human: Hola! en que me podes ayudar?\n",
      "AI: Hola! ¡Bienvenido! Estoy aquí para ayudarte en lo que necesites. ¿En qué puedo ayudarte hoy? ¿Tienes un problema que quieras resolver, una pregunta que te haga, o simplemente quieres charlar un rato? Estoy aquí para escucharte y ayudarte de la mejor manera posible.\n",
      "Human: Necesito implementar un sistema RAG (retrieval augmented generation). Cuales serían los pasos a seguir?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "user_question = 'Necesito implementar un sistema RAG (retrieval augmented generation). Cuales serían los pasos a seguir?'\n",
    "response = conversation.predict(human_input=user_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b41d4d0-205e-40b3-8c10-e90819f851ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'¡Interesante proyecto! Un sistema RAG (Retrieval-Augmented Generation) es un enfoque de inteligencia artificial que combina la búsqueda de texto (retrieval) con la generación de texto (generation) para producir resultados más precisos y relevantes. A continuación, te presento los pasos generales para implementar un sistema RAG:\\n\\n**1. Definir el objetivo y el problema**: Antes de empezar, debes definir el objetivo y el problema que deseas resolver con tu sistema RAG. ¿Qué tipo de texto quieres generar? ¿Qué tipo de búsqueda quieres realizar? ¿Qué características deseas que tenga el texto generado?\\n\\n**2. Preparar el conjunto de datos**: Necesitarás un conjunto de datos que contenga texto relevante para tu objetivo. Puedes utilizar datasets públicos o crear uno propio. El conjunto de datos debe ser lo suficientemente grande para entrenar un modelo de aprendizaje automático.\\n\\n**3. Crear un modelo de búsqueda (retrieval model)**: El modelo de búsqueda se encarga de encontrar texto relevante en el conjunto de datos que se ajuste a la query (consulta) proporcionada. Puedes utilizar modelos de aprendizaje automático como BERT, RoBERTa o DistilBERT para crear un modelo de búsqueda.\\n\\n**4. Crear un modelo de generación (generation model)**: El modelo de generación se encarga de generar texto a partir del texto relevante devuelto por el modelo de búsqueda. Puedes utilizar modelos de lenguaje como Transformers o seq2seq para crear un modelo de generación.\\n\\n**5. Entrenar el modelo de búsqueda y generación**: Entrena el modelo de búsqueda y generación utilizando el conjunto de datos preparado. Puedes utilizar algoritmos de optimización como Adam o SGD para entrenar los modelos.\\n\\n**6. Implementar la lógica de RAG**: La lógica de RAG se encarga de combinar los resultados del modelo de búsqueda y generación. Puedes implementar la lógica de RAG utilizando un algoritmo comoattenion-based or pointer-based.\\n\\n**7. Evaluar el sistema RAG**: Evalúa el sistema RAG utilizando métricas como precisión, recall, F1-score o BLEU score. Ajusta el modelo de búsqueda y generación según sea necesario para mejorar las performance del sistema.\\n\\n**8. Integrar el sistema RAG en una aplicación**: Finalmente, integra el sistema RAG en una aplicación o plataforma que lo utilice.\\n\\nRecuerda que la implementación de un sistema RAG es un proceso complejo que requiere experiencia en inteligencia artificial, lenguaje natural y programación. Si no tienes experiencia previa, es posible que necesites consultar tutoriales, documentación y recursos adicionales para implementar el sistema.\\n\\n¿Tienes alguna pregunta específica sobre alguno de estos pasos?'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "21fe42d5-e9d9-438c-b2b7-81477641caf3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
